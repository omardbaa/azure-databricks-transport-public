{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d430b9-6b2e-49ac-8e44-31a048d2e720",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d9e330-03cf-4a61-85c2-081296b55206",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Integrate and manage Public Transport data\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d190495d-198d-4c1c-8b5d-740d159c50d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    spark.conf.set(\n",
    "        f\"fs.azure.account.key.omardbstorageaccount.dfs.core.windows.net\", \n",
    "        \"f5k2suTwYCzz5Pj55WliSmjaKqyZDZ5jy88logUNbHEFNZT60n30rS/hDUIKyUdyQYdm5xMj+Kh/+AStKL6jKQ==\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6585d1b1-1d0f-4403-8060-23bbe267cd44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"abfss://publictransportdata@omardbstorageaccount.dfs.core.windows.net/raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d91938-1e09-46d3-90ea-e64540aa2240",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"inferSchema\", \"True\").option(\"header\",\n",
    "\"True\").option(\"delimeter\",\",\").load(file_location)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04d6010-623c-428d-95c3-ff0de4e63362",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, length, when, col\n",
    "\n",
    "\n",
    "arrival_condition = (length(col(\"ArrivalTime\")) == 8)\n",
    "\n",
    "\n",
    "df = df.withColumn(\"ArrivalTime\", when(arrival_condition, date_format(df[\"ArrivalTime\"], \"HH:mm\")).otherwise(df[\"ArrivalTime\"]))\n",
    "\n",
    "\n",
    "\n",
    "# Format the DepartureTime column\n",
    "df = df.withColumn(\"DepartureTime\", date_format(df[\"DepartureTime\"], \"HH:mm\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb697d5f-c272-4bc2-9adb-1aec197850d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841318b8-cb67-432f-8d31-aa2c04745f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, dayofweek\n",
    "\n",
    "df = df.withColumn(\"Year\", year(df[\"Date\"]))\n",
    "df = df.withColumn(\"Month\", month(df[\"Date\"]))\n",
    "df = df.withColumn(\"Day\", dayofmonth(df[\"Date\"]))\n",
    "df = df.withColumn(\"DayOfWeek\", dayofweek(df[\"Date\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7145eab3-5431-493e-a3c0-7493ccd31a9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, col\n",
    "\n",
    "# Convert DepartureTime and ArrivalTime columns to timestamp\n",
    "df = df.withColumn(\"DepartureTimeTimestamp\", unix_timestamp(df[\"DepartureTime\"], \"HH:mm\").cast(\"timestamp\"))\n",
    "df = df.withColumn(\"ArrivalTimeTimestamp\", unix_timestamp(df[\"ArrivalTime\"], \"HH:mm\").cast(\"timestamp\"))\n",
    "\n",
    "# Calculate TripDuration in minutes and hours\n",
    "df = df.withColumn(\"TripDurationMinutes\", (col(\"ArrivalTimeTimestamp\").cast(\"long\") - col(\"DepartureTimeTimestamp\").cast(\"long\")) / 60) # Duration in minutes\n",
    "\n",
    "\n",
    "# Drop the intermediate timestamp columns\n",
    "df = df.drop(\"DepartureTimeTimestamp\", \"ArrivalTimeTimestamp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ed1b31-affe-46fb-ac39-791cf2c0978e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\"DelayCategory\", \n",
    "    when(col(\"Delay\") == 0, \"Pas de Retard\")\n",
    "    .when((col(\"Delay\") >= 1) & (col(\"Delay\") <= 10), \"Retard Court\")\n",
    "    .when((col(\"Delay\") >= 11) & (col(\"Delay\") <= 20), \"Retard Moyen\")\n",
    "    .when(col(\"Delay\") > 20, \"Long Retard\")\n",
    "    .otherwise(\"Unknown\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "703afada-e13b-413e-96e8-de9284246b2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "\n",
    "# Assuming a threshold of 50 passengers for peak hours\n",
    "threshold = 50\n",
    "\n",
    "df = df.withColumn(\"HeureDePointe\", when(col(\"Passengers\") >= threshold, \"Peak\").otherwise(\"Off-Peak\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea51495-05ec-44d1-89f5-29a253b10876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "route_analysis = df.groupBy(\"Route\").agg(\n",
    "    F.avg(\"Delay\").alias(\"AverageDelay\"),\n",
    "    F.avg(\"Passengers\").alias(\"AveragePassengers\"),\n",
    "    F.count(\"*\").alias(\"TotalTrips\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4781a0e6-6a54-40b3-91dc-fe94f0820aeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "route_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d19839f6-317e-4d4c-8492-5bc3a95484dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join route-level statistics with the original DataFrame on the \"Route\" column\n",
    "df = df.join(route_analysis, on=\"Route\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a10ac603-9f62-432e-98da-3a46025356c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the file location for export\n",
    "#file_location = \"abfss://publictransportdata@omardbstorageaccount.dfs.core.windows.net/processed/\"\n",
    "\n",
    "# Export the DataFrame to the specified location in CSV format\n",
    "#df.write.csv(file_location, header=True, mode=\"overwrite\")\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Public Trabsport",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
