{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d9e330-03cf-4a61-85c2-081296b55206",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Integrate and manage Public Transport data\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17244067-073b-4951-8cc1-af454f170bc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.omardbstorageaccount.dfs.core.windows.net\", \n",
    "    \"n/M6dGAvjc8505kkOdZWCcfky+UKdckTs1aAhVFqb/J7Ck1yTg+meYzFXNAb/oz1mxXbizdVQdB5+ASt8wSp4w==\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95247fd2-a19d-41b1-8655-6ac2371119ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format, year, month, dayofmonth, dayofweek, unix_timestamp, when, col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Montage du Data Lake\n",
    "storageAccountName = \"omardbstorageaccount\"\n",
    "storageAccountAccessKey = \"n/M6dGAvjc8505kkOdZWCcfky+UKdckTs1aAhVFqb/J7Ck1yTg+meYzFXNAb/oz1mxXbizdVQdB5+ASt8wSp4w==\"\n",
    "sasToken = \"?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2023-09-28T15:43:37Z&st=2023-09-27T07:43:37Z&spr=https&sig=13kFGISyoUEheYmR4g9YC3qa4hCoO8YDOEQzIorKfCw%3D\"\n",
    "blobContainerName = \"public-transport-data\"\n",
    "mountPoint = \"/mnt/public-transport-data/\"\n",
    "\n",
    "# Vérification de l'existence du point de montage\n",
    "if not any(mount.mountPoint == mountPoint for mount in dbutils.fs.mounts()):\n",
    "    try:\n",
    "        dbutils.fs.mount(\n",
    "            source=\"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n",
    "            mount_point=mountPoint,\n",
    "            extra_configs={'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n",
    "        )\n",
    "        print(\"Montage réussi !\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception lors du montage :\", e)\n",
    "else:\n",
    "    print(\"Le point de montage existe déjà.\")\n",
    "\n",
    "# Définition du répertoire d'entrée dans le Data Lake Storage Gen2\n",
    "raw_data_dir = \"abfss://public-transport-data@omardbstorageaccount.dfs.core.windows.net/raw/\"\n",
    "\n",
    "# Définition du répertoire de sortie local\n",
    "local_output_dir = \"/dbfs/mnt/public-transport-data/processed/\"\n",
    "\n",
    "# Liste des mois et des noms de fichiers correspondants\n",
    "months = [\"January\", \"February\", \"March\", \"April\", \"May\"]\n",
    "\n",
    "# Définition de la taille du lot et de l'intervalle de sommeil en secondes\n",
    "batch_size = 2\n",
    "sleep_interval = 30\n",
    "\n",
    "# Traitement des fichiers par lots de deux\n",
    "for i in range(0, len(months), batch_size):\n",
    "    batch = months[i:i+batch_size]  # Get a batch of files\n",
    "    for month_name in batch:\n",
    "        # Définition des chemins des fichiers d'entrée et de sortie locaux\n",
    "        input_file = os.path.join(raw_data_dir, f\"public_transport_data_{month_name}.csv\")\n",
    "        local_output_file = os.path.join(local_output_dir, f\"public_transport_data_{month_name}_cleaned.csv\")\n",
    "\n",
    "        # Check if the processed file already exists, and if it does, skip processing\n",
    "        if os.path.exists(local_output_file):\n",
    "            print(f\"Fichier  {local_output_file} déjà traité. Ignorer...\")\n",
    "            continue\n",
    "\n",
    "        # Lecture des données\n",
    "        df = spark.read.csv(input_file, header=True, inferSchema=True)\n",
    "\n",
    "        # Conversion de \"ArrivalTime\" et \"DepartureTime\" au format \"HH:mm\"\n",
    "        df = df.withColumn(\"ArrivalTime\", date_format(df[\"ArrivalTime\"], \"HH:mm\"))\n",
    "        df = df.withColumn(\"DepartureTime\", date_format(df[\"DepartureTime\"], \"HH:mm\"))\n",
    "\n",
    "        # Ajout des colonnes Année, Mois, Jour et JourDeLaSemaine\n",
    "        df = df.withColumn(\"Year\", year(df[\"Date\"]))\n",
    "        df = df.withColumn(\"Month\", month(df[\"Date\"]))\n",
    "        df = df.withColumn(\"Day\", dayofmonth(df[\"Date\"]))\n",
    "        df = df.withColumn(\"DayOfWeek\", dayofweek(df[\"Date\"]))\n",
    "\n",
    "        # Conversion des colonnes HeureDepart et HeureArrivee en horodatage\n",
    "        df = df.withColumn(\"DepartureTimeTimestamp\", unix_timestamp(df[\"DepartureTime\"], \"HH:mm\").cast(\"timestamp\"))\n",
    "        df = df.withColumn(\"ArrivalTimeTimestamp\", unix_timestamp(df[\"ArrivalTime\"], \"HH:mm\").cast(\"timestamp\"))\n",
    "\n",
    "        # Calcul de la durée du trajet en minutes et en heures\n",
    "        df = df.withColumn(\"TripDurationMinutes\", \n",
    "            when(col(\"ArrivalTimeTimestamp\") >= col(\"DepartureTimeTimestamp\"), \n",
    "                (col(\"ArrivalTimeTimestamp\").cast(\"long\") - col(\"DepartureTimeTimestamp\").cast(\"long\")) / 60)\n",
    "            .otherwise(1440 - (col(\"DepartureTimeTimestamp\").cast(\"long\") - col(\"ArrivalTimeTimestamp\").cast(\"long\")) / 60))\n",
    "\n",
    "        # Suppression des colonnes de timestamp intermédiaires\n",
    "        df = df.drop(\"DepartureTimeTimestamp\", \"ArrivalTimeTimestamp\")\n",
    "\n",
    "        # Ajout de la colonne \"CatégorieRetard\"\n",
    "        df = df.withColumn(\"DelayCategory\", \n",
    "            when(col(\"Delay\") == 0, \"Pas de Retard\")\n",
    "            .when((col(\"Delay\") >= 1) & (col(\"Delay\") <= 10), \"Retard Court\")\n",
    "            .when((col(\"Delay\") >= 11) & (col(\"Delay\") <= 20), \"Retard Moyen\")\n",
    "            .when(col(\"Delay\") > 20, \"Long Retard\")\n",
    "            .otherwise(\"Unknown\"))\n",
    "\n",
    "        # Analyse des itinéraires \n",
    "        route_analysis = df.groupBy(\"Route\").agg(\n",
    "            F.round(F.avg(\"Delay\"), 2).alias(\"AverageDelay\"),\n",
    "            F.when((F.avg(\"Passengers\") % 1) >= 0.5, F.ceil(F.avg(\"Passengers\"))).otherwise(F.floor(F.avg(\"Passengers\"))).cast(\"int\").alias(\"AveragePassengers\"),\n",
    "            F.count(\"*\").alias(\"TotalTrips\")\n",
    "        )\n",
    "\n",
    "        df = df.join(route_analysis, on=\"Route\", how=\"left\")\n",
    "\n",
    "        # Colonne \"HeureDePointe\"\n",
    "        threshold = 50\n",
    "        df = df.withColumn(\"HeureDePointe\", when(col(\"Passengers\") >= threshold, \"Peak\").otherwise(\"Off-Peak\"))\n",
    "\n",
    "        data = df.toPandas()\n",
    "        data.to_csv(local_output_file, index=False)\n",
    "        print(f\"Traitement terminé et enregistré dans {local_output_file}\")\n",
    "\n",
    "    if i + batch_size < len(months):\n",
    "        print(f\"Pausing for {sleep_interval} seconds before the next batch...\")\n",
    "        time.sleep(sleep_interval)\n",
    "\n",
    "# Démontage du Data Lake\n",
    "dbutils.fs.unmount(mountPoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f172ecc-4562-412a-83b0-47fcda467a5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Description des Données\n",
    "\n",
    "L'ensemble de données utilisé dans ce script est destiné à la gestion des données de transports publics. Il comprend des informations sur les départs, les arrivées, les retards, les passagers, les itinéraires, et plus encore, pour plusieurs mois.\n",
    "\n",
    "## Transformations\n",
    "\n",
    "Les transformations suivantes sont appliquées aux données brutes :\n",
    "\n",
    "1. **Conversion des Horaires** : Les colonnes \"ArrivalTime\" et \"DepartureTime\" sont converties au format \"HH:mm\" pour représenter l'heure d'arrivée et de départ respectivement.\n",
    "\n",
    "2. **Ajout de Colonnes de Date** : Les colonnes \"Year\", \"Month\", \"Day\" et \"DayOfWeek\" sont ajoutées pour décomposer les dates et faciliter les analyses basées sur le temps.\n",
    "\n",
    "3. **Conversion en Horodatage** : Les colonnes \"DepartureTime\" et \"ArrivalTime\" sont converties en horodatages pour permettre des calculs de durée de trajet précis.\n",
    "\n",
    "4. **Calcul de la Durée de Trajet** : La colonne \"TripDurationMinutes\" est calculée en minutes en fonction des horodatages de départ et d'arrivée. Une gestion spéciale est effectuée pour les trajets qui passent minuit.\n",
    "\n",
    "5. **Catégorisation des Retards** : La colonne \"DelayCategory\" est ajoutée pour catégoriser les retards en \"Pas de Retard,\" \"Retard Court,\" \"Retard Moyen,\" \"Long Retard,\" ou \"Inconnu.\"\n",
    "\n",
    "6. **Analyse des Itinéraires** : Une analyse des itinéraires est effectuée pour calculer la moyenne des retards, le nombre moyen de passagers et le nombre total de trajets par itinéraire.\n",
    "\n",
    "7. **Identification des Heures de Pointe** : La colonne \"HeureDePointe\" est ajoutée pour identifier les heures de pointe en fonction du nombre de passagers.\n",
    "\n",
    "## Lignage des Données\n",
    "\n",
    "Les données sont générées à partir d'un script Python qui simule les données de transport public. Les données générées sont stockées dans le stockage Data Lake Azure à l'emplacement spécifié dans \"raw.\" Elles représentent des informations sur les départs, les arrivées, les retards, les passagers, les itinéraires, etc., pour plusieurs mois. Les sources simulées comprennent différents modes de transport public, tels que les bus, les trains, les tramways et les métros. Ces données sont ensuite traitées à l'aide d'Azure Databricks pour effectuer diverses transformations et analyses.\n",
    "\n",
    "## Directives d'Utilisation\n",
    "\n",
    "Les données traitées peuvent être utilisées pour divers cas d'utilisation, notamment :\n",
    "\n",
    "- Analyse des horaires de transport public.\n",
    "- Suivi des retards et de la ponctualité.\n",
    "- Analyse de l'utilisation des itinéraires.\n",
    "- Identification des heures de pointe.\n",
    "- Études sur l'impact de la météo sur les transports publics.\n",
    "\n",
    "Il est important de prendre en compte les catégories de retard pour une meilleure compréhension des performances du système de transport. Il est également important de noter que les données sont générées synthétiquement à des fins de démonstration et de test, ce qui peut entraîner des variations dans les données simulées.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Public-Transport-transformation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
