{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d9e330-03cf-4a61-85c2-081296b55206",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Integrate and manage Public Transport data\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17244067-073b-4951-8cc1-af454f170bc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.omardbstorageaccount.dfs.core.windows.net\", \n",
    "    \"G3TTTGHer5ZdNR78TdQvASgi9Mw+jDXH8Uk/6558WdDETLX3h3z/yGKvzS4YvfBL5ulvlBcVBvy/+ASt2O0TFA==\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9578e21f-b8d5-4d93-833c-dca786acc115",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql.functions import date_format, year, month, dayofmonth, dayofweek, unix_timestamp, when, col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define input and output directories in Data Lake Storage Gen2\n",
    "raw_data_dir = \"abfss://public-transport-data@omardbstorageaccount.dfs.core.windows.net/raw/\"\n",
    "processed_data_dir = \"abfss://public-transport-data@omardbstorageaccount.dfs.core.windows.net/processed/\"\n",
    "\n",
    "# List of months and corresponding file names\n",
    "months = [\"January\", \"February\", \"March\", \"April\", \"May\"]\n",
    "\n",
    "for month_name in months:\n",
    "    # Define input and output file paths\n",
    "    input_file = os.path.join(raw_data_dir, f\"public_transport_data_{month_name}.csv\")\n",
    "    output_file = os.path.join(processed_data_dir, f\"public_transport_data_{month_name}_cleaned.csv\")\n",
    "\n",
    "    # Read the data\n",
    "    df = spark.read.csv(input_file, header=True, inferSchema=True)\n",
    "\n",
    "    # Your data transformations here\n",
    "    # For example, convert \"ArrivalTime\" and \"DepartureTime\" to \"HH:mm\" format\n",
    "    df = df.withColumn(\"ArrivalTime\", date_format(df[\"ArrivalTime\"], \"HH:mm\"))\n",
    "    df = df.withColumn(\"DepartureTime\", date_format(df[\"DepartureTime\"], \"HH:mm\"))\n",
    "\n",
    "    # Add columns for Year, Month, Day, and DayOfWeek\n",
    "    df = df.withColumn(\"Year\", year(df[\"Date\"]))\n",
    "    df = df.withColumn(\"Month\", month(df[\"Date\"]))\n",
    "    df = df.withColumn(\"Day\", dayofmonth(df[\"Date\"]))\n",
    "    df = df.withColumn(\"DayOfWeek\", dayofweek(df[\"Date\"]))\n",
    "\n",
    "    # Convert DepartureTime and ArrivalTime columns to timestamp\n",
    "    df = df.withColumn(\"DepartureTimeTimestamp\", unix_timestamp(df[\"DepartureTime\"], \"HH:mm\").cast(\"timestamp\"))\n",
    "    df = df.withColumn(\"ArrivalTimeTimestamp\", unix_timestamp(df[\"ArrivalTime\"], \"HH:mm\").cast(\"timestamp\"))\n",
    "\n",
    "    # Calculate TripDuration in minutes and hours\n",
    "    df = df.withColumn(\"TripDurationMinutes\", \n",
    "        when(col(\"ArrivalTimeTimestamp\") >= col(\"DepartureTimeTimestamp\"), \n",
    "            (col(\"ArrivalTimeTimestamp\").cast(\"long\") - col(\"DepartureTimeTimestamp\").cast(\"long\")) / 60)\n",
    "        .otherwise(1440 - (col(\"DepartureTimeTimestamp\").cast(\"long\") - col(\"ArrivalTimeTimestamp\").cast(\"long\")) / 60))\n",
    "\n",
    "    # Drop the intermediate timestamp columns\n",
    "    df = df.drop(\"DepartureTimeTimestamp\", \"ArrivalTimeTimestamp\")\n",
    "\n",
    "    # Add DelayCategory column\n",
    "    df = df.withColumn(\"DelayCategory\", \n",
    "        when(col(\"Delay\") == 0, \"Pas de Retard\")\n",
    "        .when((col(\"Delay\") >= 1) & (col(\"Delay\") <= 10), \"Retard Court\")\n",
    "        .when((col(\"Delay\") >= 11) & (col(\"Delay\") <= 20), \"Retard Moyen\")\n",
    "        .when(col(\"Delay\") > 20, \"Long Retard\")\n",
    "        .otherwise(\"Unknown\"))\n",
    "\n",
    "    # Route analysis (you may need to define route_analysis DataFrame)\n",
    "    route_analysis = df.groupBy(\"Route\").agg(\n",
    "        F.round(F.avg(\"Delay\"), 2).alias(\"AverageDelay\"),\n",
    "        F.when((F.avg(\"Passengers\") % 1) >= 0.5, F.ceil(F.avg(\"Passengers\"))).otherwise(F.floor(F.avg(\"Passengers\"))).cast(\"int\").alias(\"AveragePassengers\"),\n",
    "        F.count(\"*\").alias(\"TotalTrips\")\n",
    "    )\n",
    "\n",
    "    df = df.join(route_analysis, on=\"Route\", how=\"left\")\n",
    "\n",
    "    # HeureDePointe column\n",
    "    threshold = 50\n",
    "    df = df.withColumn(\"HeureDePointe\", when(col(\"Passengers\") >= threshold, \"Peak\").otherwise(\"Off-Peak\"))\n",
    "\n",
    "    # Save the transformed DataFrame to the processed directory\n",
    "    df.write.option(\"header\", \"true\").csv(output_file)\n",
    "\n",
    "    # Pause execution for 2 minutes (120 seconds) before processing the next batch\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d190495d-198d-4c1c-8b5d-740d159c50d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ff1e96-90a6-42ce-a19e-3b2428116641",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format, year, month, dayofmonth, dayofweek, unix_timestamp, when, col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Mounting data lake\n",
    "storageAccountName = \"omardbstorageaccount\"\n",
    "storageAccountAccessKey = \"G3TTTGHer5ZdNR78TdQvASgi9Mw+jDXH8Uk/6558WdDETLX3h3z/yGKvzS4YvfBL5ulvlBcVBvy/+ASt2O0TFA==\"\n",
    "sasToken = \"?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2023-09-28T20:08:26Z&st=2023-09-26T12:08:26Z&spr=https&sig=7V6Sv9GschHeTzoMg80MHxy1kCl%2FPi7SnVDBozsYMTY%3D\"\n",
    "blobContainerName = \"public-transport-data\"\n",
    "mountPoint = \"/mnt/public-transport-data/\"\n",
    "\n",
    "if not any(mount.mountPoint == mountPoint for mount in dbutils.fs.mounts()):\n",
    "    try:\n",
    "        dbutils.fs.mount(\n",
    "            source=\"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n",
    "            mount_point=mountPoint,\n",
    "            extra_configs={'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n",
    "        )\n",
    "        print(\"Mount succeeded!\")\n",
    "    except Exception as e:\n",
    "        print(\"Mount exception:\", e)\n",
    "else:\n",
    "    print(\"Mount point already exists.\")\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"PublicTransportDataProcessing\").getOrCreate()\n",
    "\n",
    "# Define input directory in Data Lake Storage Gen2\n",
    "raw_data_dir = \"abfss://public-transport-data@omardbstorageaccount.dfs.core.windows.net/raw/\"\n",
    "\n",
    "# Define local output directory\n",
    "local_output_dir = \"/dbfs/mnt/public-transport-data/processed/\"\n",
    "\n",
    "# List of months and corresponding file names\n",
    "months = [\"January\", \"February\", \"March\", \"April\", \"May\"]\n",
    "\n",
    "for month_name in months:\n",
    "    # Define input and local output file paths\n",
    "    input_file = os.path.join(raw_data_dir, f\"public_transport_data_{month_name}.csv\")\n",
    "    local_output_file = os.path.join(local_output_dir, f\"public_transport_data_{month_name}_cleaned.csv\")\n",
    "\n",
    "    # Read the data\n",
    "    df = spark.read.csv(input_file, header=True, inferSchema=True)\n",
    "\n",
    "    # Your data transformations here\n",
    "    # For example, convert \"ArrivalTime\" and \"DepartureTime\" to \"HH:mm\" format\n",
    "    df = df.withColumn(\"ArrivalTime\", date_format(df[\"ArrivalTime\"], \"HH:mm\"))\n",
    "    df = df.withColumn(\"DepartureTime\", date_format(df[\"DepartureTime\"], \"HH:mm\"))\n",
    "\n",
    "    # Add columns for Year, Month, Day, and DayOfWeek\n",
    "    df = df.withColumn(\"Year\", year(df[\"Date\"]))\n",
    "    df = df.withColumn(\"Month\", month(df[\"Date\"]))\n",
    "    df = df.withColumn(\"Day\", dayofmonth(df[\"Date\"]))\n",
    "    df = df.withColumn(\"DayOfWeek\", dayofweek(df[\"Date\"]))\n",
    "\n",
    "    # Convert DepartureTime and ArrivalTime columns to timestamp\n",
    "    df = df.withColumn(\"DepartureTimeTimestamp\", unix_timestamp(df[\"DepartureTime\"], \"HH:mm\").cast(\"timestamp\"))\n",
    "    df = df.withColumn(\"ArrivalTimeTimestamp\", unix_timestamp(df[\"ArrivalTime\"], \"HH:mm\").cast(\"timestamp\"))\n",
    "\n",
    "    # Calculate TripDuration in minutes and hours\n",
    "    df = df.withColumn(\"TripDurationMinutes\", \n",
    "        when(col(\"ArrivalTimeTimestamp\") >= col(\"DepartureTimeTimestamp\"), \n",
    "            (col(\"ArrivalTimeTimestamp\").cast(\"long\") - col(\"DepartureTimeTimestamp\").cast(\"long\")) / 60)\n",
    "        .otherwise(1440 - (col(\"DepartureTimeTimestamp\").cast(\"long\") - col(\"ArrivalTimeTimestamp\").cast(\"long\")) / 60))\n",
    "\n",
    "    # Drop the intermediate timestamp columns\n",
    "    df = df.drop(\"DepartureTimeTimestamp\", \"ArrivalTimeTimestamp\")\n",
    "\n",
    "    # Add DelayCategory column\n",
    "    df = df.withColumn(\"DelayCategory\", \n",
    "        when(col(\"Delay\") == 0, \"Pas de Retard\")\n",
    "        .when((col(\"Delay\") >= 1) & (col(\"Delay\") <= 10), \"Retard Court\")\n",
    "        .when((col(\"Delay\") >= 11) & (col(\"Delay\") <= 20), \"Retard Moyen\")\n",
    "        .when(col(\"Delay\") > 20, \"Long Retard\")\n",
    "        .otherwise(\"Unknown\"))\n",
    "\n",
    "    # Route analysis (you may need to define route_analysis DataFrame)\n",
    "    route_analysis = df.groupBy(\"Route\").agg(\n",
    "        F.round(F.avg(\"Delay\"), 2).alias(\"AverageDelay\"),\n",
    "        F.when((F.avg(\"Passengers\") % 1) >= 0.5, F.ceil(F.avg(\"Passengers\"))).otherwise(F.floor(F.avg(\"Passengers\"))).cast(\"int\").alias(\"AveragePassengers\"),\n",
    "        F.count(\"*\").alias(\"TotalTrips\")\n",
    "    )\n",
    "\n",
    "    df = df.join(route_analysis, on=\"Route\", how=\"left\")\n",
    "\n",
    "    # HeureDePointe column\n",
    "    threshold = 50\n",
    "    df = df.withColumn(\"HeureDePointe\", when(col(\"Passengers\") >= threshold, \"Peak\").otherwise(\"Off-Peak\"))\n",
    "\n",
    "    data = df.toPandas()\n",
    "    data.to_csv(local_output_file, index=False)\n",
    "    print(\"done\")\n",
    "    \n",
    "    # Pause execution for 2 minutes (120 seconds) before processing the next batch\n",
    "    time.sleep(10)\n",
    "# Unmount the data lake\n",
    "dbutils.fs.unmount(mountPoint)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Public Transport",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
